{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7b93fa-ebc3-42a2-991f-4723e6d23cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# !pip install requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e254f080-ff34-4eee-88b9-e2d5d5456755",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSS_ISSNS = [\n",
    "    # Online ISSNs (E-ISSNs) \n",
    "    \"2432-2725\",  # Journal of Computational Social Science (online)  \n",
    "    \"2053-9517\",  # Big Data & Society (online)                       \n",
    "    \"2193-1127\",  # EPJ Data Science (online)                          \n",
    "    \"2197-4314\",  # Computational Social Networks (online)             \n",
    "]\n",
    "\n",
    "SS_ISSNS = [\n",
    "    \"1537-5390\",  # American Journal of Sociology (online)             \n",
    "    \"1467-954X\",  # The Sociological Review (online)                   \n",
    "    \"1573-7853\",  # Theory & Society (online)                          \n",
    "    \"2003-1998\",  # Journal of Digital Social Resources (online)       \n",
    "]\n",
    "\n",
    "YEARS = list(range(2020, 2026))         \n",
    "N_PER_STRATUM = 20                      \n",
    "RANDOM_SEED_BASE = 20251018              \n",
    "CROSSREF_MAILTO = \"shujashakir@gmail.com\"  \n",
    "USE_SEMANTIC_SCHOLAR = True              \n",
    "PAUSE = 0.2                              \n",
    "OUT_DIR = \"out_sampling\"                 \n",
    "\n",
    "\n",
    "import time, random, requests\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "CR_BASE = \"https://api.crossref.org/works\"\n",
    "SS_BASE = \"https://api.semanticscholar.org/graph/v1/paper/DOI:\"\n",
    "\n",
    "@dataclass\n",
    "class Row:\n",
    "    doi: str\n",
    "    title: str\n",
    "    journal: str\n",
    "    year: int\n",
    "    abstract: str\n",
    "    stratum: str  # \"CSS\" or \"SS\"\n",
    "    url: str\n",
    "\n",
    "def crossref_query(issn: str, year: int, rows: int = 1000):\n",
    "    params = {\n",
    "        \"filter\": f\"type:journal-article,from-pub-date:{year}-01-01,until-pub-date:{year}-12-31,issn:{issn}\",\n",
    "        \"rows\": rows,\n",
    "        \"mailto\": CROSSREF_MAILTO,\n",
    "        \"select\": \"DOI,title,container-title,issued,type,URL,abstract\"\n",
    "    }\n",
    "    url = f\"{CR_BASE}?{urlencode(params)}\"\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"message\", {}).get(\"items\", [])\n",
    "\n",
    "def ss_abstract(doi: str) -> str:\n",
    "    url = f\"{SS_BASE}{doi}?fields=title,abstract,venue,year\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=20)\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get(\"abstract\") or \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def normalize_title(t):\n",
    "    if isinstance(t, list) and t:\n",
    "        return t[0]\n",
    "    return t or \"\"\n",
    "\n",
    "def normalize_journal(j):\n",
    "    if isinstance(j, list) and j:\n",
    "        return j[0]\n",
    "    return j or \"\"\n",
    "\n",
    "def issued_year(issued):\n",
    "    try:\n",
    "        return int(issued[\"date-parts\"][0][0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_frame(issns, label):\n",
    "    frame, seen = [], set()\n",
    "    for issn in issns:\n",
    "        for y in YEARS:\n",
    "            try:\n",
    "                items = crossref_query(issn, y)\n",
    "                time.sleep(PAUSE)\n",
    "            except Exception:\n",
    "                continue\n",
    "            for it in items:\n",
    "                if it.get(\"type\") != \"journal-article\":\n",
    "                    continue\n",
    "                doi = (it.get(\"DOI\") or \"\").strip()\n",
    "                if not doi or doi in seen:\n",
    "                    continue\n",
    "                seen.add(doi)\n",
    "                title   = normalize_title(it.get(\"title\"))\n",
    "                journal = normalize_journal(it.get(\"container-title\"))\n",
    "                year    = issued_year(it.get(\"issued\"))\n",
    "                abstract= (it.get(\"abstract\") or \"\").strip()\n",
    "                url     = (it.get(\"URL\") or \"\").strip()\n",
    "                frame.append(Row(doi, title, journal, year, abstract, label, url))\n",
    "    return frame\n",
    "\n",
    "def fill_missing_abstracts(rows):\n",
    "    if not USE_SEMANTIC_SCHOLAR:\n",
    "        return rows\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        if not r.abstract:\n",
    "            abs2 = ss_abstract(r.doi)\n",
    "            if abs2:\n",
    "                r = Row(r.doi, r.title, r.journal, r.year, abs2, r.stratum, r.url)\n",
    "            time.sleep(0.15)\n",
    "        out.append(r)\n",
    "    return out\n",
    "\n",
    "def clean_frame(rows):\n",
    "    out, seen = [], set()\n",
    "    for r in rows:\n",
    "        if not r.doi or r.doi in seen:         continue\n",
    "        if not r.title or not r.journal or not r.year: continue\n",
    "        if not r.abstract:                     continue\n",
    "        seen.add(r.doi)\n",
    "        out.append(r)\n",
    "    return out\n",
    "\n",
    "def deterministic_sample(frame, n, seed):\n",
    "    rng = random.Random(seed)\n",
    "    if len(frame) < n:\n",
    "        raise ValueError(f\"Stratum has {len(frame)} items; need {n}. Add ISSNs or widen years.\")\n",
    "    return rng.sample(frame, n)\n",
    "\n",
    "def write_csv(path: Path, rows, header=None):\n",
    "    df = pd.DataFrame(rows) if isinstance(rows, list) else rows\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b72b3cd-d049-4b04-bc80-324a7223b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame sizes → CSS: 1173, SS: 761\n",
      "\n",
      "Wrote: out_sampling\\pilot_candidates.csv\n",
      "Open it, skim quickly, and set keep=0 for non-research/irrelevant entries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# sanity check\n",
    "if not CSS_ISSNS or not SS_ISSNS:\n",
    "    raise SystemExit(\"Please fill CSS_ISSNS and SS_ISSNS with ONLINE (e-ISSN) values before running.\")\n",
    "\n",
    "out_dir = Path(OUT_DIR)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# build frames\n",
    "css_frame = clean_frame(fill_missing_abstracts(build_frame(CSS_ISSNS, \"CSS\")))\n",
    "ss_frame  = clean_frame(fill_missing_abstracts(build_frame(SS_ISSNS, \"SS\")))\n",
    "\n",
    "css_pop, ss_pop = len(css_frame), len(ss_frame)\n",
    "print(f\"Frame sizes → CSS: {css_pop}, SS: {ss_pop}\")\n",
    "\n",
    "# reproducible samples\n",
    "css_sample = deterministic_sample(css_frame, N_PER_STRATUM, RANDOM_SEED_BASE)\n",
    "ss_sample  = deterministic_sample(ss_frame,  N_PER_STRATUM, RANDOM_SEED_BASE + 1)\n",
    "\n",
    "# candidates table\n",
    "def row_to_dict(idx, r):\n",
    "    return {\n",
    "        \"id\": idx,\n",
    "        \"doi\": r.doi,\n",
    "        \"title\": r.title,\n",
    "        \"journal\": r.journal,\n",
    "        \"year\": r.year,\n",
    "        \"abstract\": r.abstract,\n",
    "        \"stratum\": r.stratum,\n",
    "        \"url\": r.url,\n",
    "        \"keep\": 1,   # ← after human review, set to 0 to drop\n",
    "    }\n",
    "\n",
    "cand = []\n",
    "i = 1\n",
    "for r in css_sample: cand.append(row_to_dict(i, r)); i += 1\n",
    "for r in ss_sample:  cand.append(row_to_dict(i, r)); i += 1\n",
    "\n",
    "pilot_candidates_path = out_dir / \"pilot_candidates.csv\"\n",
    "write_csv(pilot_candidates_path, cand)\n",
    "print(f\"\\nWrote: {pilot_candidates_path}\\n\"\n",
    "      f\"Open it, skim quickly, and set keep=0 for non-research/irrelevant entries.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff4b6633-52b8-4618-a332-34b7d058353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSS] ISSN 2432-2725\n",
      "  year 2020: fetched 36 items (capped=120)\n",
      "  year 2021: fetched 57 items (capped=120)\n",
      "  year 2022: fetched 41 items (capped=120)\n",
      "  year 2023: fetched 38 items (capped=120)\n",
      "  year 2024: fetched 114 items (capped=120)\n",
      "  year 2025: fetched 79 items (capped=120)\n",
      "[CSS] ISSN 2053-9517\n",
      "  year 2020: fetched 86 items (capped=120)\n",
      "  year 2021: fetched 105 items (capped=120)\n",
      "  year 2022: fetched 93 items (capped=120)\n",
      "  year 2023: fetched 120 items (capped=120)\n",
      "  year 2024: fetched 108 items (capped=120)\n",
      "  year 2025: fetched 100 items (capped=120)\n",
      "[CSS] ISSN 2193-1127\n",
      "  year 2020: fetched 36 items (capped=120)\n",
      "  year 2021: fetched 57 items (capped=120)\n",
      "  year 2022: fetched 62 items (capped=120)\n",
      "  year 2023: fetched 63 items (capped=120)\n",
      "  year 2024: fetched 77 items (capped=120)\n",
      "  year 2025: fetched 75 items (capped=120)\n",
      "[CSS] ISSN 2197-4314\n",
      "  year 2021: fetched 22 items (capped=120)\n",
      "  year 2020: fetched 6 items (capped=120)\n",
      "[CSS] raw frame size: 1375\n",
      "[CSS] cleaned frame size: 1079\n",
      "[SS] ISSN 1537-5390\n",
      "  year 2020: fetched 120 items (capped=120)\n",
      "  year 2021: fetched 120 items (capped=120)\n",
      "  year 2022: fetched 120 items (capped=120)\n",
      "  year 2023: fetched 120 items (capped=120)\n",
      "  year 2024: fetched 120 items (capped=120)\n",
      "  year 2025: fetched 103 items (capped=120)\n",
      "[SS] ISSN 1467-954X\n",
      "  year 2020: fetched 85 items (capped=120)\n",
      "  year 2021: fetched 70 items (capped=120)\n",
      "  year 2022: fetched 82 items (capped=120)\n",
      "  year 2023: fetched 94 items (capped=120)\n",
      "  year 2024: fetched 73 items (capped=120)\n",
      "  year 2025: fetched 83 items (capped=120)\n",
      "[SS] ISSN 1573-7853\n",
      "  year 2020: fetched 50 items (capped=120)\n",
      "  year 2021: fetched 47 items (capped=120)\n",
      "  year 2022: fetched 36 items (capped=120)\n",
      "  year 2023: fetched 27 items (capped=120)\n",
      "  year 2024: fetched 51 items (capped=120)\n",
      "  year 2025: fetched 60 items (capped=120)\n",
      "[SS] ISSN 2003-1998\n",
      "  year 2020: fetched 15 items (capped=120)\n",
      "  year 2021: fetched 22 items (capped=120)\n",
      "  year 2022: fetched 16 items (capped=120)\n",
      "  year 2023: fetched 25 items (capped=120)\n",
      "  year 2024: fetched 39 items (capped=120)\n",
      "  year 2025: fetched 11 items (capped=120)\n",
      "[SS] raw frame size: 1589\n",
      "[SS] cleaned frame size: 688\n",
      "\n",
      "Frame sizes → CSS: 1079, SS: 688\n",
      "\n",
      "WROTE → out_sampling\\pilot_candidates.csv\n",
      "Audit log updated → out_sampling\\sampling_log.csv\n",
      "Next: open pilot_candidates.csv, mark keep=0 where needed, then run FINALIZE cell.\n"
     ]
    }
   ],
   "source": [
    "# FULL 20+20 papers\n",
    "\n",
    "from pathlib import Path\n",
    "import time, random, pandas as pd\n",
    "\n",
    "USE_SEMANTIC_SCHOLAR = False   # speed: fill missing abstracts later if needed\n",
    "N_PER_STRATUM = 20\n",
    "YEARS = list(range(2020, 2026))  # 2020–2025\n",
    "\n",
    "# Cap CSN (Computational Social Networks, e-ISSN 2197-4314) \n",
    "YEARS_BY_ISSN = {\"2197-4314\": [2021, 2020]}\n",
    "\n",
    "def years_for_issn(issn: str):\n",
    "    return YEARS_BY_ISSN.get(issn, YEARS)\n",
    "\n",
    "def build_frame_with_overrides_verbose(issns, label, per_year_limit=120):\n",
    "    \"\"\"Fetch CrossRef items per ISSN/year (with optional per-year cap).\"\"\"\n",
    "    frame, seen = [], set()\n",
    "    for issn in issns:\n",
    "        print(f\"[{label}] ISSN {issn}\")\n",
    "        for y in years_for_issn(issn):\n",
    "            try:\n",
    "                items = crossref_query(issn, y, rows=per_year_limit)\n",
    "                print(f\"  year {y}: fetched {len(items)} items (capped={per_year_limit})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  year {y}: ERROR -> {e}\")\n",
    "                items = []\n",
    "            time.sleep(PAUSE)\n",
    "            for it in items:\n",
    "                if it.get(\"type\") != \"journal-article\":\n",
    "                    continue\n",
    "                doi = (it.get(\"DOI\") or \"\").strip()\n",
    "                if not doi or doi in seen:\n",
    "                    continue\n",
    "                seen.add(doi)\n",
    "                frame.append(Row(\n",
    "                    doi=doi,\n",
    "                    title=normalize_title(it.get(\"title\")),\n",
    "                    journal=normalize_journal(it.get(\"container-title\")),\n",
    "                    year=issued_year(it.get(\"issued\")),\n",
    "                    abstract=(it.get(\"abstract\") or \"\").strip(),\n",
    "                    stratum=label,\n",
    "                    url=(it.get(\"URL\") or \"\").strip()\n",
    "                ))\n",
    "    print(f\"[{label}] raw frame size: {len(frame)}\")\n",
    "\n",
    "    # Fill abstracts \n",
    "    rows = fill_missing_abstracts(frame) if USE_SEMANTIC_SCHOLAR else frame\n",
    "    cleaned = clean_frame(rows)\n",
    "    print(f\"[{label}] cleaned frame size: {len(cleaned)}\")\n",
    "    return cleaned\n",
    "\n",
    "out_dir = Path(OUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "css_frame = build_frame_with_overrides_verbose(CSS_ISSNS, \"CSS\", per_year_limit=120)\n",
    "ss_frame  = build_frame_with_overrides_verbose(SS_ISSNS,  \"SS\",  per_year_limit=120)\n",
    "\n",
    "css_pop, ss_pop = len(css_frame), len(ss_frame)\n",
    "print(f\"\\nFrame sizes → CSS: {css_pop}, SS: {ss_pop}\")\n",
    "\n",
    "# Deterministic stratified samples (raise helpful error if too small)\n",
    "def must_sample(frame, n, seed):\n",
    "    if len(frame) < n:\n",
    "        raise ValueError(\n",
    "            f\"Not enough items to sample {n}.\\n\"\n",
    "            f\"Have {len(frame)} in this stratum. Consider adding another ISSN or widening years.\"\n",
    "        )\n",
    "    rng = random.Random(seed)\n",
    "    return rng.sample(frame, n)\n",
    "\n",
    "css_sample = must_sample(css_frame, N_PER_STRATUM, RANDOM_SEED_BASE)\n",
    "ss_sample  = must_sample(ss_frame,  N_PER_STRATUM, RANDOM_SEED_BASE + 1)\n",
    "\n",
    "def row_to_dict(idx, r):\n",
    "    return {\n",
    "        \"id\": idx,\n",
    "        \"doi\": r.doi,\n",
    "        \"title\": r.title,\n",
    "        \"journal\": r.journal,\n",
    "        \"year\": r.year,\n",
    "        \"abstract\": r.abstract,\n",
    "        \"stratum\": r.stratum,\n",
    "        \"url\": r.url,\n",
    "        \"keep\": 1,  # set to 0 during manual verification\n",
    "    }\n",
    "\n",
    "cand = []\n",
    "i = 1\n",
    "for r in css_sample: cand.append(row_to_dict(i, r)); i += 1\n",
    "for r in ss_sample:  cand.append(row_to_dict(i, r)); i += 1\n",
    "\n",
    "pilot_candidates_path = out_dir / \"pilot_candidates.csv\"\n",
    "pd.DataFrame(cand).to_csv(pilot_candidates_path, index=False)\n",
    "print(f\"\\nWROTE → {pilot_candidates_path}\")\n",
    "\n",
    "# Minimal audit log \n",
    "log_path = out_dir / \"sampling_log.csv\"\n",
    "log_entry = pd.DataFrame([{\n",
    "    \"phase\": \"discover_sample\",\n",
    "    \"population_css\": css_pop,\n",
    "    \"population_ss\": ss_pop,\n",
    "    \"random_seed_base\": RANDOM_SEED_BASE,\n",
    "    \"timestamp\": int(time.time()),\n",
    "}])\n",
    "if log_path.exists():\n",
    "    log_df = pd.read_csv(log_path)\n",
    "    log_df = pd.concat([log_df, log_entry], ignore_index=True)\n",
    "else:\n",
    "    log_df = log_entry\n",
    "log_df.to_csv(log_path, index=False)\n",
    "print(f\"Audit log updated → {log_path}\\n\"\n",
    "      f\"Next: open pilot_candidates.csv, mark keep=0 where needed, then run FINALIZE cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49027d56-ad87-4a77-a527-4543bcee3398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: out_sampling\\pilot_candidates.csv\n",
      "Rows kept: 40 of 40\n",
      "WROTE → out_sampling\\pilot_40.csv\n",
      "Audit log updated → out_sampling\\sampling_log.csv\n"
     ]
    }
   ],
   "source": [
    "# FINALIZE VERIFIED SAMPLE\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd, time\n",
    "\n",
    "out_dir = Path(OUT_DIR)\n",
    "cand_path = out_dir / \"pilot_candidates.csv\"\n",
    "final_path = out_dir / \"pilot_40.csv\"\n",
    "\n",
    "print(f\"Loading: {cand_path}\")\n",
    "df = pd.read_csv(cand_path)\n",
    "\n",
    "# Keep only manually approved rows\n",
    "df_keep = df[df[\"keep\"] == 1].copy()\n",
    "print(f\"Rows kept: {len(df_keep)} of {len(df)}\")\n",
    "\n",
    "# Sort by stratum for readability\n",
    "df_keep = df_keep.sort_values(by=[\"stratum\", \"journal\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "# Write the final dataset\n",
    "df_keep.to_csv(final_path, index=False)\n",
    "print(f\"WROTE → {final_path}\")\n",
    "\n",
    "# Append audit log\n",
    "log_path = out_dir / \"sampling_log.csv\"\n",
    "entry = pd.DataFrame([{\n",
    "    \"phase\": \"finalize\",\n",
    "    \"timestamp\": int(time.time()),\n",
    "    \"num_kept\": len(df_keep),\n",
    "    \"num_discarded\": len(df) - len(df_keep)\n",
    "}])\n",
    "if log_path.exists():\n",
    "    old = pd.read_csv(log_path)\n",
    "    pd.concat([old, entry], ignore_index=True).to_csv(log_path, index=False)\n",
    "else:\n",
    "    entry.to_csv(log_path, index=False)\n",
    "print(f\"Audit log updated → {log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
